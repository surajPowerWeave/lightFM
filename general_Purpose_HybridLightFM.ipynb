{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries  \n",
    "First we’ll need to import some libraries and a number from the LightFM library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lightfm.data import Dataset\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from lightfm.evaluation import auc_score\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from scipy.sparse import coo_matrix as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data  \n",
    "To train the model using LightFM we will supply the following:  \n",
    " 1. interactions as a sparse.coo_matrix – our interactions will be UseGuiD and ProductGuid\n",
    " 2. user_features as an iterable of strings containing user features – Company Size,Industry Type, location, etc\n",
    " 3. item_features as an iterable of strings containing item features – Product Category, Color, etc\n",
    " \n",
    "LightFM has a dataset constructor with a number of handy methods to get our data ready to input into the model. As we want to include user and item features in our model, preparing the data will be a two step process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create the feature mappings\n",
    "First, we use the fit() method on the dataset object to create the mappings.  \n",
    "Second, can call the build_interactions() method on the dataset object to build the interactions matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'user/customer'\n",
    "dataset = Dataset()\n",
    "df_interaction = order_dframe_model.copy()\n",
    "arr2D_interaction = df_interaction[[subject,'ProductGuid','ratings']].to_numpy()\n",
    "del df_interaction\n",
    "dataset.fit(users=df_interaction[subject].unique(), items=df_interaction.ProductId.unique())\n",
    "(interactions, weights) = dataset.build_interactions((tuple(row) for row in arr2D_interaction))\n",
    "num_users, num_items = dataset.interactions_shape()\n",
    "print(‘Num users: {}, num_items {}.’.format(num_users, num_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the dataset.fit_partial() method to create the feature mappings for user_features and model_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2D_userFeature = feature_user.to_numpy()\n",
    "arr2D_itemFeature = feature_item.to_numpy()\n",
    "del feature_user\n",
    "del feature_item\n",
    "dataset.fit_partial(users=(row[‘UserGuid’] for row in arr2D_userFeature), \n",
    "                    items=(row[‘ProductGuid’] for row in arr2D_itemFeature), \n",
    "                    item_features=(row[‘ParentColectionGuid’] for row in arr2D_itemFeature), \n",
    "                    user_features=(row[‘Location’] for row in arr2D_userFeature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build the interactions, user feature and item feature matrices:  \n",
    "We call the build_item_features() and build_user_features() methods on the dataset object to build the item_features and user_features. These return objects of type sparse.coo_matrix as required by LightFM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = dataset.build_item_features(((row[‘ProductGuid’], [row[‘ParentCategory’]]) for row in arr2D_itemFeature))\n",
    "user_features = dataset.build_user_features(((row[‘UserGuid’], [row[‘location’]]) for row in arr2D_userFeature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the Model  \n",
    "\n",
    "To train the recommender we will use the Weighted Approximate Rank Pair-Wise (WARP) loss function provided in the LightFM library. WARP deals with (user, positive item, negative item) triplets.\n",
    "LightFM provides the following explanation of the WARP algorithm:\n",
    "1. For a given (user, positive item pair), sample a negative item at random from all the remaining items. Compute predictions for both items; if the negative item’s prediction exceeds that of the positive item plus a margin, perform a gradient update to rank the positive item higher and the negative item lower. If there is no rank violation, continue sampling negative items until a violation is found.\n",
    "2. If you found a violating negative example at the first try, make a large gradient update: this indicates that a lot of negative items are ranked higher than positives items given the current state of the model, and the model must be updated by a large amount. If it took a lot of sampling to find a violating example, perform a small update: the model is likely close to the optimum and should be updated at a low rate.  \n",
    "\n",
    "Here’s how we specify the model with the WARP loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model = LightFM(loss=’warp’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be specified with numerous hyperparameters by importing lightFM_bestParameter.ipnb file. It is worth mentioning that LightFM also allows Bayesian Personalised Ranking loss, though this routinely performs less well.  \n",
    "\n",
    "## Training the Model  \n",
    "\n",
    "We can then call .fit() to fit the model to the interactions, the item and user feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(interactions, item_features=item_features, user_features=user_features, epochs=2, sample_weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once, model is save and retrieve then we can use fit_partial considering only rest of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model.fit_partial(partial_interactions, etc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
